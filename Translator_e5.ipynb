{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkxMSgGC3TkNz6d85d6eh2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angeruPpb/IA_Transformer/blob/main/Translator_e5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OmG_P5YJJFuX"
      },
      "outputs": [],
      "source": [
        "# Ejecutar para importar keras-transformer\n",
        "# pip install keras-transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras_transformer import get_model, decode\n",
        "from pickle import load\n",
        "from google.colab import drive\n",
        "\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "Zz2fe6hqKYDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "filename = '/content/drive/My Drive/Personal/Datasets/english-spanish.pkl'\n",
        "dataset = load(open(filename, 'rb'))\n",
        "print(dataset[120000,0])\n",
        "print(dataset[120000,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuUBZczLNLMe",
        "outputId": "5def9c9f-83ab-4a22-ad64-2f6642f20389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "tom is a new yorker but he doesnt have a new york accent\n",
            "tom es neoyorquino pero no tiene acento de nueva york\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear tokens\n",
        "source_tokens = []\n",
        "for sentence in dataset[:,0]:\n",
        "  source_tokens.append(sentence.split(' '))\n",
        "print(source_tokens[120000])\n",
        "\n",
        "target_tokens = []\n",
        "for sentence in dataset[:,1]:\n",
        "  target_tokens.append(sentence.split(' '))\n",
        "print(target_tokens[120000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbmqFWVyOxd4",
        "outputId": "a5d5f1e0-c649-4709-a377-5496aa7051e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tom', 'is', 'a', 'new', 'yorker', 'but', 'he', 'doesnt', 'have', 'a', 'new', 'york', 'accent']\n",
            "['tom', 'es', 'neoyorquino', 'pero', 'no', 'tiene', 'acento', 'de', 'nueva', 'york']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_token_dictionary(token_list):\n",
        "  token_dict = {\n",
        "      '<PAD>': 0,\n",
        "      '<START>': 1,\n",
        "      '<END>':2\n",
        "  }\n",
        "  for tokens in token_list:\n",
        "    for token in tokens:\n",
        "      if token not in token_dict:\n",
        "        token_dict[token] = len(token_dict)\n",
        "  return token_dict"
      ],
      "metadata": {
        "id": "qHHIhcQFPi9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asignamos un valor numerico a cada token/palabra en el dataset\n",
        "source_token_dict = build_token_dictionary(source_tokens)\n",
        "target_token_dict = build_token_dictionary(target_tokens)\n",
        "target_token_dict_inv = {v:k for k,v in target_token_dict.items()}\n",
        "\n",
        "# print(source_token_dict)\n",
        "# print(target_token_dict)\n",
        "# print(target_token_dict_inv)"
      ],
      "metadata": {
        "id": "2inftv4lRXr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar Start,End y Pad a cada frase del set de entrenamiento\n",
        "encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
        "decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
        "output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
        "\n",
        "source_max_len = max(map(len, encoder_tokens))\n",
        "target_max_len = max(map(len, decoder_tokens))\n",
        "\n",
        "encoder_tokens = [tokens + ['<PAD>']*(source_max_len-len(tokens)) for tokens in encoder_tokens]\n",
        "decoder_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in decoder_tokens]\n",
        "output_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in output_tokens]"
      ],
      "metadata": {
        "id": "qJfIufjmS6dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asi quedan los nuevos tokens\n",
        "print(encoder_tokens[120000])\n",
        "print(decoder_tokens[120000])\n",
        "print(output_tokens[120000])"
      ],
      "metadata": {
        "id": "4T9PoCtBUr4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164bba27-86cc-42d6-ecb4-a11b7454e93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<START>', 'go', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['<START>', 've', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['ve', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encoder_tokens]\n",
        "decoder_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decoder_tokens]\n",
        "output_decoded = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in output_tokens]\n",
        "\n",
        "print(encoder_input[120000])\n",
        "print(decoder_input[120000])\n",
        "print(output_decoded[120000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYkSXV3PVOrs",
        "outputId": "95f46e52-8837-4f97-e963-6d6722d1c4ca"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 258, 120, 197, 12666, 2914, 32, 1577, 140, 120, 197, 5385, 4287, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 123, 387, 23929, 1210, 45, 1006, 8223, 98, 1233, 10238, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[123, 387, 23929, 1210, 45, 1006, 8223, 98, 1233, 10238, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear Transformer\n",
        "model = get_model(\n",
        "    token_num= max(len(source_token_dict), len(target_token_dict)),\n",
        "    embed_dim = 32,         # Entrada del vector Embedding\n",
        "    encoder_num = 2,        # Cant. de bloques de codificacion, son 6\n",
        "    decoder_num = 2,        # Cant. de bloques de decodificacion, son 6\n",
        "    head_num = 4,           # Cant. de bloques atencionales, son 8\n",
        "    hidden_dim = 128,       # Cant. de neuronas en la red Neuronal de cada bloque, son 2048\n",
        "    dropout_rate = 0.05,    # Porcentaje de neuropnas que se desactivan para evitar Overffiting\n",
        "    use_same_embed = False, # La representacion de las frases en ingles y español son diferentes\n",
        ")\n",
        "\n",
        "model.compile('adam', 'sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lna2chwWgio",
        "outputId": "26f99935-175b-4706-da4b-40467fc630dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " Encoder-Input (InputLayer)  [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " Encoder-Token-Embedding (E  [(None, None, 32),           808608    ['Encoder-Input[0][0]']       \n",
            " mbeddingRet)                 (25269, 32)]                                                        \n",
            "                                                                                                  \n",
            " Encoder-Embedding (TrigPos  (None, None, 32)             0         ['Encoder-Token-Embedding[0][0\n",
            " Embedding)                                                         ]']                           \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAtt  (None, None, 32)             4224      ['Encoder-Embedding[0][0]']   \n",
            " ention (MultiHeadAttention                                                                       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAtt  (None, None, 32)             0         ['Encoder-1-MultiHeadSelfAtten\n",
            " ention-Dropout (Dropout)                                           tion[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAtt  (None, None, 32)             0         ['Encoder-Embedding[0][0]',   \n",
            " ention-Add (Add)                                                    'Encoder-1-MultiHeadSelfAtten\n",
            "                                                                    tion-Dropout[0][0]']          \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAtt  (None, None, 32)             64        ['Encoder-1-MultiHeadSelfAtten\n",
            " ention-Norm (LayerNormaliz                                         tion-Add[0][0]']              \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward (Fee  (None, None, 32)             8352      ['Encoder-1-MultiHeadSelfAtten\n",
            " dForward)                                                          tion-Norm[0][0]']             \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Drop  (None, None, 32)             0         ['Encoder-1-FeedForward[0][0]'\n",
            " out (Dropout)                                                      ]                             \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Add   (None, None, 32)             0         ['Encoder-1-MultiHeadSelfAtten\n",
            " (Add)                                                              tion-Norm[0][0]',             \n",
            "                                                                     'Encoder-1-FeedForward-Dropou\n",
            "                                                                    t[0][0]']                     \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Norm  (None, None, 32)             64        ['Encoder-1-FeedForward-Add[0]\n",
            "  (LayerNormalization)                                              [0]']                         \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAtt  (None, None, 32)             4224      ['Encoder-1-FeedForward-Norm[0\n",
            " ention (MultiHeadAttention                                         ][0]']                        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Decoder-Input (InputLayer)  [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAtt  (None, None, 32)             0         ['Encoder-2-MultiHeadSelfAtten\n",
            " ention-Dropout (Dropout)                                           tion[0][0]']                  \n",
            "                                                                                                  \n",
            " Decoder-Token-Embedding (E  [(None, None, 32),           808608    ['Decoder-Input[0][0]']       \n",
            " mbeddingRet)                 (25269, 32)]                                                        \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAtt  (None, None, 32)             0         ['Encoder-1-FeedForward-Norm[0\n",
            " ention-Add (Add)                                                   ][0]',                        \n",
            "                                                                     'Encoder-2-MultiHeadSelfAtten\n",
            "                                                                    tion-Dropout[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-Embedding (TrigPos  (None, None, 32)             0         ['Decoder-Token-Embedding[0][0\n",
            " Embedding)                                                         ]']                           \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAtt  (None, None, 32)             64        ['Encoder-2-MultiHeadSelfAtten\n",
            " ention-Norm (LayerNormaliz                                         tion-Add[0][0]']              \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAtt  (None, None, 32)             4224      ['Decoder-Embedding[0][0]']   \n",
            " ention (MultiHeadAttention                                                                       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward (Fee  (None, None, 32)             8352      ['Encoder-2-MultiHeadSelfAtten\n",
            " dForward)                                                          tion-Norm[0][0]']             \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAtt  (None, None, 32)             0         ['Decoder-1-MultiHeadSelfAtten\n",
            " ention-Dropout (Dropout)                                           tion[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Drop  (None, None, 32)             0         ['Encoder-2-FeedForward[0][0]'\n",
            " out (Dropout)                                                      ]                             \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAtt  (None, None, 32)             0         ['Decoder-Embedding[0][0]',   \n",
            " ention-Add (Add)                                                    'Decoder-1-MultiHeadSelfAtten\n",
            "                                                                    tion-Dropout[0][0]']          \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Add   (None, None, 32)             0         ['Encoder-2-MultiHeadSelfAtten\n",
            " (Add)                                                              tion-Norm[0][0]',             \n",
            "                                                                     'Encoder-2-FeedForward-Dropou\n",
            "                                                                    t[0][0]']                     \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAtt  (None, None, 32)             64        ['Decoder-1-MultiHeadSelfAtten\n",
            " ention-Norm (LayerNormaliz                                         tion-Add[0][0]']              \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Norm  (None, None, 32)             64        ['Encoder-2-FeedForward-Add[0]\n",
            "  (LayerNormalization)                                              [0]']                         \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAt  (None, None, 32)             4224      ['Decoder-1-MultiHeadSelfAtten\n",
            " tention (MultiHeadAttentio                                         tion-Norm[0][0]',             \n",
            " n)                                                                  'Encoder-2-FeedForward-Norm[0\n",
            "                                                                    ][0]',                        \n",
            "                                                                     'Encoder-2-FeedForward-Norm[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAt  (None, None, 32)             0         ['Decoder-1-MultiHeadQueryAtte\n",
            " tention-Dropout (Dropout)                                          ntion[0][0]']                 \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAt  (None, None, 32)             0         ['Decoder-1-MultiHeadSelfAtten\n",
            " tention-Add (Add)                                                  tion-Norm[0][0]',             \n",
            "                                                                     'Decoder-1-MultiHeadQueryAtte\n",
            "                                                                    ntion-Dropout[0][0]']         \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAt  (None, None, 32)             64        ['Decoder-1-MultiHeadQueryAtte\n",
            " tention-Norm (LayerNormali                                         ntion-Add[0][0]']             \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward (Fee  (None, None, 32)             8352      ['Decoder-1-MultiHeadQueryAtte\n",
            " dForward)                                                          ntion-Norm[0][0]']            \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward-Drop  (None, None, 32)             0         ['Decoder-1-FeedForward[0][0]'\n",
            " out (Dropout)                                                      ]                             \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward-Add   (None, None, 32)             0         ['Decoder-1-MultiHeadQueryAtte\n",
            " (Add)                                                              ntion-Norm[0][0]',            \n",
            "                                                                     'Decoder-1-FeedForward-Dropou\n",
            "                                                                    t[0][0]']                     \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward-Norm  (None, None, 32)             64        ['Decoder-1-FeedForward-Add[0]\n",
            "  (LayerNormalization)                                              [0]']                         \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAtt  (None, None, 32)             4224      ['Decoder-1-FeedForward-Norm[0\n",
            " ention (MultiHeadAttention                                         ][0]']                        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAtt  (None, None, 32)             0         ['Decoder-2-MultiHeadSelfAtten\n",
            " ention-Dropout (Dropout)                                           tion[0][0]']                  \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAtt  (None, None, 32)             0         ['Decoder-1-FeedForward-Norm[0\n",
            " ention-Add (Add)                                                   ][0]',                        \n",
            "                                                                     'Decoder-2-MultiHeadSelfAtten\n",
            "                                                                    tion-Dropout[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAtt  (None, None, 32)             64        ['Decoder-2-MultiHeadSelfAtten\n",
            " ention-Norm (LayerNormaliz                                         tion-Add[0][0]']              \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAt  (None, None, 32)             4224      ['Decoder-2-MultiHeadSelfAtten\n",
            " tention (MultiHeadAttentio                                         tion-Norm[0][0]',             \n",
            " n)                                                                  'Encoder-2-FeedForward-Norm[0\n",
            "                                                                    ][0]',                        \n",
            "                                                                     'Encoder-2-FeedForward-Norm[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAt  (None, None, 32)             0         ['Decoder-2-MultiHeadQueryAtte\n",
            " tention-Dropout (Dropout)                                          ntion[0][0]']                 \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAt  (None, None, 32)             0         ['Decoder-2-MultiHeadSelfAtten\n",
            " tention-Add (Add)                                                  tion-Norm[0][0]',             \n",
            "                                                                     'Decoder-2-MultiHeadQueryAtte\n",
            "                                                                    ntion-Dropout[0][0]']         \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAt  (None, None, 32)             64        ['Decoder-2-MultiHeadQueryAtte\n",
            " tention-Norm (LayerNormali                                         ntion-Add[0][0]']             \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward (Fee  (None, None, 32)             8352      ['Decoder-2-MultiHeadQueryAtte\n",
            " dForward)                                                          ntion-Norm[0][0]']            \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward-Drop  (None, None, 32)             0         ['Decoder-2-FeedForward[0][0]'\n",
            " out (Dropout)                                                      ]                             \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward-Add   (None, None, 32)             0         ['Decoder-2-MultiHeadQueryAtte\n",
            " (Add)                                                              ntion-Norm[0][0]',            \n",
            "                                                                     'Decoder-2-FeedForward-Dropou\n",
            "                                                                    t[0][0]']                     \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward-Norm  (None, None, 32)             64        ['Decoder-2-FeedForward-Add[0]\n",
            "  (LayerNormalization)                                              [0]']                         \n",
            "                                                                                                  \n",
            " Decoder-Output (EmbeddingS  (None, None, 25269)          25269     ['Decoder-2-FeedForward-Norm[0\n",
            " im)                                                                ][0]',                        \n",
            "                                                                     'Decoder-Token-Embedding[0][1\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1701877 (6.49 MB)\n",
            "Trainable params: 1701877 (6.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento\n",
        "x = [np.array(encoder_input), np.array(decoder_input)]\n",
        "y = np.array(output_decoded)\n",
        "\n",
        "model.fit(x,y, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI2ptAsZabMR",
        "outputId": "c42ce08b-8a8d-43ab-9192-c7a35195e10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3886/3886 [==============================] - 176s 40ms/step - loss: 1.3628\n",
            "Epoch 2/5\n",
            "3886/3886 [==============================] - 152s 39ms/step - loss: 1.2631\n",
            "Epoch 3/5\n",
            "3886/3886 [==============================] - 149s 38ms/step - loss: 1.2070\n",
            "Epoch 4/5\n",
            "3886/3886 [==============================] - 147s 38ms/step - loss: 1.1686\n",
            "Epoch 5/5\n",
            "3886/3886 [==============================] - 158s 41ms/step - loss: 1.1438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d8f58618e50>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar archivo con pesos en el drive\n",
        "# model.save('/content/drive/My Drive/Personal/Transformers/translator_e5.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIEFdeokl5XW",
        "outputId": "51f95efd-b766-4891-b17e-6526284cfcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# newfile = '/content/drive/My Drive/Personal/Transformers/translator_e5.h5'\n",
        "# La funcion load_weights es propia de keras-transformer\n",
        "# model.load_weights(newfile)"
      ],
      "metadata": {
        "id": "KXtS0VPW3awI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "  sentence_tokens = [tokens + ['<END>', '<PAD>'] for tokens in [sentence.split(' ')]]\n",
        "  transformer_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in sentence_tokens][0]\n",
        "  print('tr_input: ', transformer_input)\n",
        "  decoded = decode(\n",
        "      model,\n",
        "      transformer_input,\n",
        "      start_token = target_token_dict['<START>'],\n",
        "      end_token = target_token_dict['<END>'],\n",
        "      pad_token = target_token_dict['<PAD>']\n",
        "  )\n",
        "\n",
        "  print('Frase original: {}'.format(sentence))\n",
        "  print(decoded[1:-1])\n",
        "  print('Traduccion: {}'.format(' '.join(map(lambda x: target_token_dict_inv[x], decoded[1:-1]))))\n"
      ],
      "metadata": {
        "id": "76h0H9fLbaSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('the red car')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En_GSkTwfSkf",
        "outputId": "fef5789a-685d-4991-82c6-a326687ad966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tr_input:  [750, 199, 748, 2, 0]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Frase original: the red car\n",
            "[67, 1568, 5027]\n",
            "Traduccion: el coche roja\n"
          ]
        }
      ]
    }
  ]
}